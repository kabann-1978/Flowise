{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjT2YZefBkqP8nozFebZj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kabann-1978/Flowise/blob/main/exemplo_rede_neural_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "HVAP37X-Y7Bs"
      },
      "outputs": [],
      "source": [
        "# exemplo de rede neural que reconhece dígitos\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() # define a conversão de imagem para tensor, pq se deixar em jpeg o desempenho não é tão bom como em \"tensor\"\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=36, shuffle=True) # cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) #carrega a parte de validadção do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=36, shuffle=True) # cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "a4VA7yNweBel"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exibe uma imagem da importação\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = next(dataiter)\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Zb9CDVCVdJpH",
        "outputId": "a52154de-9903-4018-aa6b-bfd5961de059"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) # verifica as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) # verifica as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sLfhhCJnbew",
        "outputId": "03101a42-ec77-433e-e801-3efe7d68d5a2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a estrutura da rede neural\n",
        "\n",
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # camada de entrada, 784 neurôneos que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neurôneos que se ligam a 64\n",
        "    self.lienar3 = nn.Linear(64, 10) # camada interna 2, 64 neurôneos que se ligam a 10\n",
        "\n",
        "# para a camada de saída não é necessário definir nada, pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "def forward(self, x):\n",
        "  x = F.relu(self.linear1(x)) # função que ativa a camada de entrada para a camada interna 1\n",
        "  x = F.relu(self.linear2(x)) # função que ativa a camada interna 1 para a camada 2\n",
        "  x = F.self.lienar3(x) # função que ativa a camada interna 2 para a camada de saída (f(x)=x)\n",
        "  return F.log_softmax(x,dim=1) # dados usados para cálculo da perda\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmXHAKyFoF05"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define o modelo de treinamento da rede neural\n",
        "\n",
        "def treino(modelo, treinloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a política de atualização dos pesos e das bias\n",
        "  inicio = time() # timer definido para saber quanto tempo levou o treino da rede\n",
        "\n",
        "  criterio = nn.NLLLoss() # define o critério para calcular a perda\n",
        "  EPOCHS = 10 # número constante de epochs que o algoritmo irá processar (ideal é pelo menos 100, porém, leva muito tempo, para efeito de estudo está somente 10)\n",
        "  modelo.train() # ativa o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # converte as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis\n",
        "      otimizador.zero_grad() # zera os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # coloca os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calcula a perda da epoch em utilizada no momento\n",
        "\n",
        "      perda_instantanea.backward() # back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() # atualiza os pesos e as bías\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) \",(time()-inicio)/60)\n"
      ],
      "metadata": {
        "id": "twyMSao82sCZ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compara uma imagem não treinada com as imagens treinadas\n",
        "\n",
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1,784)\n",
        "\n",
        "      # Desativa o autograd para acelerar a validação. Os grafos computacionais possuem um alto custo computacional\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo de escala logarítimica\n",
        "\n",
        "\n",
        "      ps = torch.exp(logps) # converte output para escala normal (lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, neste caso, o número que o modelo previu no treino\n",
        "      etiqueta_certa = etiquetas.numpy()[1]\n",
        "      if (etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "  print(\"Total de imagens testadas *\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo * {}%\".format(conta_corretas*100/conta_todas))\n"
      ],
      "metadata": {
        "id": "CXJJKY2e5vFZ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instancia o modelo\n",
        "\n",
        "modelo = Modelo()\n",
        "\n",
        "# Verifica se possui modelo cuda em GPU, se não, CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWkDrHdHzA5",
        "outputId": "baeff609-b51f-4b77-f825-94c7fa6cad99"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (lienar3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}